{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Check, if the installation is working properly</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize([1,2,3,4,5])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.map(lambda x:2*x)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> Working with text files</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets work with NASDAQ stock data for 1 year duration (28.09.2018-27.09.2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('nasdaq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing. parse the data to human readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Record = namedtuple(\"Record\", [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj_Close\", \"Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(s):\n",
    "    fields = s.split(\",\")\n",
    "    return Record(fields[0], *map(float, fields[1:6]), Volume=int(fields[6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the data and cache it to the in-memory\n",
    "parsed_data = raw_data.map(parse_record).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.map(lambda x:x.Date).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.map(lambda x:x.Date).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.map(lambda x:x.Volume).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the total volume by month data\n",
    "\n",
    "by_month_data = parsed_data.map(lambda x:(x.Date[:7],x.Volume)).reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_month_data.top(1,lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the tuples to string before saving the data to a file\n",
    "\n",
    "result_data = by_month_data.map(lambda t:\",\".join(map(str,t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data.saveAsTextFile(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Application of Join: return of everyday on NASDAQ stock price </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = sc.textFile('nasdaq.csv').map(parse_record).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the next date\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "def get_next_date(s):\n",
    "    fmt = \"%Y-%m-%d\"\n",
    "    return (datetime.strptime(s,fmt)+timedelta(days=1)).strftime(fmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-11-09'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_date(\"2018-11-08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## closing price of a given date\n",
    "\n",
    "date_and_close_price = parsed_data.map(lambda r:(r.Date,r.Close))\n",
    "\n",
    "## date_and_close_price.take(3)\n",
    "\n",
    "## closing price for previous date\n",
    "\n",
    "date_and_prev_close_price = parsed_data.map(lambda r:(get_next_date(r.Date),r.Close))\n",
    "\n",
    "## join the two prices by same key\n",
    "\n",
    "joined = date_and_close_price.join(date_and_prev_close_price)\n",
    "joined_left = date_and_close_price.leftOuterJoin(date_and_prev_close_price)\n",
    "joined_right = date_and_close_price.rightOuterJoin(date_and_prev_close_price)\n",
    "joined_full = date_and_close_price.fullOuterJoin(date_and_prev_close_price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7023.830078, 6905.919922)]\n",
      "[(7023.830078, 6905.919922)]\n",
      "[(7023.830078, 6905.919922)]\n",
      "[(7023.830078, 6905.919922)]\n"
     ]
    }
   ],
   "source": [
    "# here one can understand the difference btw different types of joins. Whenever values is not availble it will be None\n",
    "# in right case 'None' from right, in left case 'None' from left and in full both possibility, play with it to understand\n",
    "\n",
    "# note that lookup is used to search by a particular key\n",
    "\n",
    "key_date = \"2019-01-15\"\n",
    "\n",
    "print(joined.lookup(key_date))\n",
    "print(joined_right.lookup(key_date))\n",
    "print(joined_left.lookup(key_date))\n",
    "print(joined_full.lookup(key_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate return\n",
    "\n",
    "returns = joined.mapValues(lambda p:(p[0]/p[1]-1.0)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6986.069824]\n",
      "[6971.47998]\n",
      "[-0.20884194357574382]\n"
     ]
    }
   ],
   "source": [
    "date = \"2019-01-11\"\n",
    "\n",
    "print(date_and_prev_close_price.lookup(date))\n",
    "print(date_and_close_price.lookup(date))\n",
    "print(returns.lookup(date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> broadcast and accumulator variable </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accumulator variable: to check how long a code takes to persist data on external drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = sc.textFile('nasdaq.csv').map(parse_record).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import AccumulatorParam\n",
    "import time\n",
    "import random\n",
    "\n",
    "class MaxAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, initial_value):\n",
    "        return initial_value\n",
    "    def addInPlace(self, accumulator, delta):\n",
    "        return max(accumulator, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_persist = sc.accumulator(0.0, MaxAccumulatorParam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_to_external_storage(iterable):\n",
    "    for record in iterable:\n",
    "        before = time.time()\n",
    "        time.sleep(random.random()/1000.0)\n",
    "        after = time.time()\n",
    "        time_persist.add(after-before)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.foreachPartition(persist_to_external_storage)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_persist.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### broadcast varible to pass input externally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = sc.textFile(\"nasdaq.csv\").map(parse_record).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sc.broadcast({\"mu\":1910949928.057554, \"sigma\":284610509.115})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_regressor(v):\n",
    "    time.sleep(random.random()/1000.0)\n",
    "    return 0.5*((v-params.value[\"mu\"])/params.value[\"sigma\"])**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.map(lambda x:super_regressor(x.Volume)).top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use join outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'http://172.17.0.19:4040'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paste this url in your browser and toggle between different tabs to know the system conf and job details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
